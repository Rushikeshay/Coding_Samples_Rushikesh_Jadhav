# Parts of Week 7
# 1.1 

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
conda install openpyxl

provider = pd.read_csv(r"C:\Users\rushi\OneDrive - The University of Chicago\Spring 2025-Jadhav-DellXPS-13\Python 1\unformatted_medicare_post_acute_care_hospice_by_provider_and_service_2014_12_31.csv")

# CMS has data suppression rules if the values are too small. (https://resdac.org/articles/cms-cell-size-suppression-policy#:~:text=As%20outlined%20for%20researchers%20in,10%20can%20be%20reported%20directly.)
# The rule is that any table or aggregate data showing cell sizes of 10 or less 
# must be suppressed in publications and presentations. They use signs like "*" 
# or "#" indicate such observations.

# To crrect for numeric variables with "NA" and "*"

provider = pd.read_csv(
    r"C:\Users\rushi\OneDrive - The University of Chicago\Spring 2025-Jadhav-DellXPS-13\Python 1\unformatted_medicare_post_acute_care_hospice_by_provider_and_service_2014_12_31.csv",
    na_values=["NA", "*"],
    low_memory=False
    )

# The use of low_memory=False (https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options)
# When low_memory is set to False, the entire file is read into memory at once. 
# This allows pandas to analyze all the data together and infer the correct data
# types for each column, ensuring consistency. While this approach can be more 
# memory-intensive, it avoids the DtypeWarning that often occurs when low_memory
# is True and mixed data types are encountered. 
# Setting low_memory=False prioritizes accurate data type inference
# at the cost of potentially higher memory usage, while low_memory=True 
# prioritizes lower memory usage but may sacrifice data type accuracy.

assert provider.shape == (31665, 122), "Incorrect datasize :("

# 1.2 

provider_hhrg = pd.read_excel(r"C:\Users\rushi\OneDrive - The University of Chicago\Spring 2025-Jadhav-DellXPS-13\Python 1\Provider_by_HHRG_PUF.xlsx",
                              sheet_name="Data")

# We see a special note in the methods tab stating "Any aggregated records at any
# level which are derived from 10 or fewer beneficiaries are excluded to protect 
# the privacy of Medicare beneficiaries.

provider_hhrg.head()
provider_hhrg.columns
provider_hhrg.info()

# find folumns with $ 

dollar_cols = [
    col for col in provider_hhrg.columns
    if provider_hhrg[col].astype(str).str.contains(r"\$").any()
    ]

dollar_cols

# fix those with $ replace with NA make them numeric 
for col in dollar_cols:
    provider_hhrg[col] = (
        provider_hhrg[col]
        .replace('[\$,]', '', regex=True) 
        .replace('^\\s*$', pd.NA, regex=True) 
    )
    provider_hhrg[col] = pd.to_numeric(provider_hhrg[col], errors='coerce')

# check variables with $ type 

provider_hhrg[dollar_cols].dtypes

# they are all floats now yay! 

assert provider_hhrg.shape == (111904, 20), "Incorrect datasize :("

# 1.3 
case_mix_weight = pd.read_excel(r"C:\Users\rushi\OneDrive - The University of Chicago\Spring 2025-Jadhav-DellXPS-13\Python 1\CY 2014 Final HH PPS Case-Mix Weights.xlsx")

# drop 2013 column
case_mix_weight.drop(columns=["2013 HH PPS Case-Mix Weights"], inplace=True)

# rename 2014 column 
case_mix_weight.rename(columns={"2014 HH PPS Case-Mix Weights": "casemix_2014"}, inplace=True)

assert case_mix_weight.shape == (153, 4), "Incorrect shape"

# Week 8 

# 3.5 

# Two relevant columns: Payment group and 2014 Final HH PPS Case-Mix Weights
provider_hhrg.columns  
case_mix_weight.columns 
case_mix_weight["2014 Final HH PPS Case-Mix Weights"].info()

unique_groups = case_mix_weight['Payment group'].nunique()
unique_groups
# 153 groups 

# 3.6 
case_mix_weight.head(5)
case_mix_weight['Payment group'] = case_mix_weight['Payment group'].astype(str)

case_mix_weight['Grp_1'] = case_mix_weight['Payment group'].str.slice(0, 1)  
case_mix_weight['Grp_2'] = case_mix_weight['Payment group'].str.slice(1, 2)  
case_mix_weight['Grp_3'] = case_mix_weight['Payment group'].str.slice(2, 3)  
case_mix_weight['Grp_4'] = case_mix_weight['Payment group'].str.slice(3, 4) 
case_mix_weight['Grp_5'] = case_mix_weight['Payment group'].str.slice(4, 5)  

# 3.7

provider_hhrg.columns
provider_hhrg['Grpng'].head(5)

provider_hhrg['Payment group'] = provider_hhrg['Grpng'].astype(str)

provider_hhrg['Grp_1'] = provider_hhrg['Payment group'].str.slice(0, 1)
provider_hhrg['Grp_2'] = provider_hhrg['Payment group'].str.slice(1, 2)
provider_hhrg['Grp_3'] = provider_hhrg['Payment group'].str.slice(2, 3)
provider_hhrg['Grp_4'] = provider_hhrg['Payment group'].str.slice(3, 4)
provider_hhrg['Grp_5'] = provider_hhrg['Payment group'].str.slice(4, 5)

for i in range(1, 6):
    col = f'Grp_{i}'
    print({col})
    print("case mix weight:", sorted(case_mix_weight[col].dropna().unique()))
    print("provider HHRG:", sorted(provider_hhrg[col].dropna().unique()))
    print()

# Maping Grp_1 to early or late
case_mix_weight['Grp_1'] = case_mix_weight['Grp_1'].apply(lambda x: 'early' if x in ['1', '2'] else 'late')
provider_hhrg['Grp_1'] = provider_hhrg['Grp_1'].replace({'1': 'early', '2': 'early', '3': 'late', '4': 'late', '5': 'late'})

# Mapping Grp_2, Grp_3, Grp_4 as per their matches 
case_mix_weight['Grp_2'] = case_mix_weight['Grp_2'].map({'0': 'A', '1': 'B', '2': 'C'})
case_mix_weight['Grp_3'] = case_mix_weight['Grp_3'].map({'1': 'F', '2': 'G', '3': 'H'})
case_mix_weight['Grp_4'] = case_mix_weight['Grp_4'].map({'1': 'K', '2': 'L', '3': 'M'})

# Dropping Grp_5 as it a placeholder 
case_mix_weight.drop(columns=['Grp_5'], inplace=True)
provider_hhrg.drop(columns=['Grp_5'], inplace=True)

# 3.8 

# Check for duplicates in case_mix_weight
duplicates = case_mix_weight.duplicated(subset=['Grp_1', 'Grp_2', 'Grp_3', 'Grp_4'], keep=False)
duplicate_rows = case_mix_weight[duplicates].sort_values(['Grp_1', 'Grp_2', 'Grp_3', 'Grp_4'])
print(duplicate_rows)

# Keep the first match because there are duplicates in case_mix_weight 
case_mix_weight = case_mix_weight.drop_duplicates(subset=['Grp_1', 'Grp_2', 'Grp_3', 'Grp_4'])

# merge ensuring all rows from provider_hhrg are kept
provider_hhrg_wt = pd.merge(
    provider_hhrg,
    case_mix_weight,
    on=['Grp_1', 'Grp_2', 'Grp_3', 'Grp_4'],
    how='left',  
    indicator=True,  
    validate='m:1' # (many provider rows to one case mix weight)
    )

print(provider_hhrg_wt['_merge'].value_counts())

# Drop _merge
provider_hhrg_wt = provider_hhrg_wt.drop(columns=['_merge'])

# Assert that the merged dataset has the same number of rows as provider_hhrg
assert provider_hhrg_wt.shape[0] == provider_hhrg.shape[0], "Merge and HHRG don't have the same number of rows"

provider_hhrg_wt.shape
assert provider_hhrg_wt.shape[0] == 111904, "Row count does not match"

# 4.1 

# Clean and convert numeric columns
provider_hhrg_wt['Avg_Pymt_Amt_Per_Epsd'] = (
    provider_hhrg_wt['Avg_Pymt_Amt_Per_Epsd']
    .replace('[\$,]', '', regex=True)
    .astype(float)
    )
provider_hhrg_wt['Tot_Epsd_Stay_Cnt'] = pd.to_numeric(
    provider_hhrg_wt['Tot_Epsd_Stay_Cnt'], 
    errors='coerce'
    )
# drop NaNs
provider_hhrg_wt = provider_hhrg_wt.dropna(
    subset=['Avg_Pymt_Amt_Per_Epsd', 'Tot_Epsd_Stay_Cnt']
    )

# group at provider level
grouped = provider_hhrg_wt.groupby(['Prvdr_ID', 'Prvdr_Name', 'State'])

# calculate avg cost 
avg_cost = grouped.apply(
    lambda x: np.average(x['Avg_Pymt_Amt_Per_Epsd'], weights=x['Tot_Epsd_Stay_Cnt'])
    ).rename('avg_cost')

# mean case mix in 2014 weighted by total episodes
avg_case_mix = grouped.apply(
    lambda x: np.average(x['2014 Final HH PPS Case-Mix Weights'], weights=x['Tot_Epsd_Stay_Cnt'])
    ).rename('avg_case_mix')

# Sum of total episodes
total_episodes = grouped['Tot_Epsd_Stay_Cnt'].sum().rename('total_episodes')
total_episodes

# results 
provider_sum = pd.concat([avg_cost, avg_case_mix, total_episodes], axis=1).reset_index()

# shape

assert provider_sum.shape[0] == 8652, "Row counts does not match"

provider_sum.shape

# It seems that I have about ~50 more rows in my data than expected. I am not sure why that could 
# be the case. The number of columns is correct.  

provider_sum.head()

# 4.2 
# hist plot showign dist of avg cost per ep.
plt.figure()
sns.histplot(
    data=provider_sum,
    x='avg_cost',
    bins=50,
    )
plt.title('Distribution of average Cost per Episode by Provider')
plt.xlabel('Average Cost per episode (USD)')
plt.ylabel('Number of providers')
plt.show()

# cost scaterplot 
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=provider_sum,
    x='avg_case_mix',
    y='avg_cost',
    alpha=0.5
    )
plt.title('Average cost vs. Case Mix weight by provider')
plt.xlabel('Average case mix')
plt.ylabel('Average cost per episode (USD)')
plt.show()


# Thick right tail intails a small number of providers have significantly higher
# average costs than the majority. This might indicate:
# Providers treating complex and sicker cases. We can see that in the second graph
# with higher avg_case_mix on the x axis.
# There could be potential overbilling that can be categorized as fraud or 
# inefficiency and that will require further investigation.

# We know that A case-mix of 1 corresponds to a typical patient’s expected cost. 
# A score of 0.75 indicates low expected costs and a score of 1.5 indicates high 
# expected costs.
# As a result we can't claim fraud because the high costs might simply reflect 
# legitimate high price for more seviourly sick patients. 
# To claim fraud we will need to audit the claims, not just observing cost 
# distributions.

# 4.3 

plt.figure()
sns.regplot(
    data=provider_sum,
    x='avg_case_mix',
    y='avg_cost',
    line_kws={'color': 'red'} 
    )
plt.title('Average cost vs. Case mix weight by Provider')
plt.xlabel('Average Case mix weight')
plt.ylabel('Average cost per episode (USD)')
plt.show()

# There is a positive correlation. Soas avg_case_mix increases, avg_cost tends to 
# rise. This is expected because sicker patients require more services.
# BUt this graph helps us see the followings 
# Outliers: Some providers deviate from the trend where they might have high cost and 
# low case mix. This could indicate potential overbilling. Further investigation would
# be recommended to udnderstand these outliers. 
# OR the second scenario where have low cost and high case mix indicating 
# possible under treatment or inefficiency. 

# 4.4 
provider_sum['cost_normalized'] = provider_sum['avg_cost'] / provider_sum['avg_case_mix']

# 4.5 

plt.figure()
# avg_cost
sns.histplot(
    data=provider_sum,
    x='avg_cost',
    bins=50,
    color='blue',
    alpha=0.5,
    label='Avg Cost'
    )
# cost_normalized
sns.histplot(
    data=provider_sum,
    x='cost_normalized',
    bins=50,
    color='red',
    alpha=0.5,
    label='Cost normalized by case mix'
    )

plt.title('Distributions of avg cost vs. Normalized cost')
plt.xlabel('Cost per episode (USD)')
plt.ylabel('Density')
plt.legend()
plt.show()

# To better get an idea of the skewness, getting the mathamatical skewness using skew function. 
print(f"Original avg_cost skew: {provider_sum['avg_cost'].skew():.2f}\n"
    f"Normalized cost skew: {provider_sum['cost_normalized'].skew():.2f}"
    )

# Analysis 
# The original average cost distribution is skewed right by 0.90, and normalizing 
# costs by case-mix weight amplifies the skewness to 2.09.
# This indicates that adjusting for patient severity shows even more extreme 
# outliers. 
# This suggests that a subset of providers has disproportionately high costs even 
# after accounting for patient complexit and seviarity, which aligns with out idea of 
# potential fraud or inefficiency. 
# The increased skew in cost_normalized shows that case-mix adjustment doesn’t just 
# explain away high costs—it instead isolates a smaller group of providers whose 
# costs can't be justified by patient severity alone. This makes them the prime 
# candidates for further investigation. 

# 4.6 

il_raw = provider_sum[provider_sum['State'] == 'IL'].nlargest(5, 'avg_cost')[['Prvdr_Name', 'avg_cost', 'avg_case_mix']]
il_raw

il_norm = provider_sum[provider_sum['State'] == 'IL'].nlargest(5, 'cost_normalized')[['Prvdr_Name', 'cost_normalized', 'avg_case_mix']]
il_norm

# Investigators should prioritize the normalized cost group, as established in the
# previous question. 
# Just looking into the raw cost group could lead investogators to penalizing 
# providers that serve legitimately to the sicker patients, potentially reducing 
# care access for those in need. 
# The normalized variable better isolates suspicious providers and their billing 
# patterns. 
